{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import tqdm as tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into Pandas DataFrame\n",
    "\n",
    "data = pd.read_csv('dataCo.csv', encoding='ISO-8859-1')\n",
    "data.head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information about each data in the dataset\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical information for numberical variables\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = data.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numCols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data[col], kde=True)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = data.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in catCols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=data, x=col, order=data[col].value_counts().index)\n",
    "    plt.title(f'Count of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationmatrix = data[numCols].corr()\n",
    "plt.figure(figsize=(30, 18))\n",
    "sns.heatmap(correlationmatrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unrelated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "colsToDrop = [\n",
    "    'Type',\n",
    "    'Days for shipment (scheduled)',\n",
    "    'Days for shipping (real)',\n",
    "    'Delivery Status',\n",
    "    'Late_delivery_risk',\n",
    "    'Category Id',\n",
    "    'Customer City',\n",
    "    'Customer Country',\n",
    "    'Customer Email',\n",
    "    'Customer Fname',\n",
    "    'Customer Id',\n",
    "    'Customer Lname',\n",
    "    'Customer Password',\n",
    "    'Customer State',\n",
    "    'Customer Street',\n",
    "    'Customer Zipcode',\n",
    "    'Department Id',\n",
    "    'Department Name',\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'Order City',\n",
    "    'Order Country',\n",
    "    'Order Item Cardprod Id',\n",
    "    'Order Item Id',\n",
    "    'Order State',\n",
    "    'Product Description',\n",
    "    'Product Image',\n",
    "    'Product Status',\n",
    "    'Order Item Product Price',\n",
    "    'Order Item Total',\n",
    "    'Shipping Mode',\n",
    "    'Order Status',\n",
    "    'Order Zipcode',\n",
    "    'shipping date (DateOrders)',\n",
    "    'Customer Segment',\n",
    "    'Market',\n",
    "    'Order Item Discount',\n",
    "    'Order Item Discount Rate',\n",
    "    'Order Region',\n",
    "    'Product Card Id',\n",
    "    'Product Category Id',\n",
    "    'Product Name',\n",
    "    'Product Price'\n",
    "]\n",
    "# Create new DataFrame by dropping the specified columns\n",
    "newData = data.drop(columns = colsToDrop)\n",
    "newData.head().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData['order date (DateOrders)'] = pd.to_datetime(newData['order date (DateOrders)'])\n",
    "newData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the date of most recent purchase - for Recency\n",
    "\n",
    "newData['order date (DateOrders)'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set present date as next day of most recent purchase\n",
    "\n",
    "present = dt.datetime(2018,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recency = No. of days between 1/2/2018 and date of last purchase (per customer)\n",
    "### Frequency = No. of orders per customer\n",
    "### Monetary = Total purchase price per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency = Subtract maximum date from 1/2/2018 -> result converted into number of days since last purchase\n",
    "# Frequency = Count number of orders for each customer\n",
    "# Monetary = Sum of all prices paid by each customer\n",
    "# Convert columns into RFM aspects\n",
    "\n",
    "rfm = newData.groupby('Order Customer Id').agg({'order date (DateOrders)': lambda x: (present - x.max()).days, \n",
    "                                             'Order Id': lambda x: len(x), 'Sales per customer': lambda x: x.sum()})\n",
    "\n",
    "rfm['order date (DateOrders)'] = rfm['order date (DateOrders)'].astype(int)\n",
    "\n",
    "rfm.rename(columns={'order date (DateOrders)': 'Recency (R)', \n",
    "                         'Order Id': 'Frequency (F)', \n",
    "                         'Sales per customer': 'Monetary (M)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing RFM data into five percentiles\n",
    "\n",
    "quantiles = rfm.quantile(q=[0.2,0.4,0.6,0.8])\n",
    "quantiles = quantiles.to_dict()\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for score assignment\n",
    "\n",
    "# Recency is best at minimum so 1st percentile = 5\n",
    "def recency(x, y, z):\n",
    "    if x <= z[y][0.20]:\n",
    "        return 5\n",
    "    elif x <= z[y][0.40]:\n",
    "        return 4\n",
    "    elif x <= z[y][0.60]:\n",
    "        return 3\n",
    "    elif x <= z[y][0.80]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# F and M is better when score is higher so 1st percentile = 1\n",
    "def frequencyAndMonetary(a, b, c):\n",
    "    if a <= c[b][0.20]:\n",
    "        return 1\n",
    "    elif a <= c[b][0.40]:\n",
    "        return 2\n",
    "    elif a <= c[b][0.60]:\n",
    "        return 3\n",
    "    elif a <= c[b][0.80]:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for R Score to indicate the score between 1 to 5\n",
    "rfm['R Score'] = rfm['Recency (R)'].apply(recency, args=('Recency (R)',quantiles))\n",
    "\n",
    "# Create a column for F Score to indicate the score between 1 to 5\n",
    "rfm['F Score'] = rfm['Frequency (F)'].apply(frequencyAndMonetary, args=('Frequency (F)',quantiles))\n",
    "\n",
    "# Create a column for M Score to indicate the score between 1 to 5\n",
    "rfm['M Score'] = rfm['Monetary (M)'].apply(frequencyAndMonetary, args=('Monetary (M)',quantiles))\n",
    "\n",
    "rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for combined RFM score\n",
    "rfm['RFM Score'] = rfm['R Score'].astype(str)+ rfm['F Score'].astype(str) + rfm['M Score'].astype(str)\n",
    "rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of RFM attributes\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.histplot(rfm['Recency (R)'], kde=True)\n",
    "plt.xlabel('Recency')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.histplot(rfm['Frequency (F)'], kde=True)\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.histplot(rfm['Monetary (M)'], kde=True)\n",
    "plt.xlabel('Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building - RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all unique values and total count\n",
    "\n",
    "count=rfm['RFM Score'].unique()\n",
    "\n",
    "print(count)\n",
    "\n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summation of all R, F, M Scores for segmentation\n",
    "\n",
    "rfm['Total Score'] = rfm[['R Score','F Score','M Score']].sum(axis=1)\n",
    "rfm['Total Score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform segmentation based on RFM Scores (3 4 5 6 7 8 9 10 11 12 13)\n",
    "# Assign customers into 3 segments - high, mid, low value\n",
    "# 3 - 6 -> low value customers\n",
    "# 7 - 9 -> mid value customers\n",
    "# 10 - 13 -> high value customers\n",
    "\n",
    "def RFMSegmentation(df):\n",
    "    if 3 <= df['Total Score'] <= 6:\n",
    "        return 'Low Value Customers' \n",
    "    elif 7 <= df['Total Score'] <= 9:\n",
    "        return 'Mid Value Customers' \n",
    "    elif 10 <= df['Total Score'] <= 13:\n",
    "        return 'High Value Customers'\n",
    "    else:\n",
    "        return 'Invalid score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column indicating each customer segments based on RFM\n",
    "\n",
    "rfm['Customer Segmentation'] =rfm.apply(RFMSegmentation, axis=1)\n",
    "rfm.head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot customer segment distribution in a pie chart\n",
    "\n",
    "rfm['Customer Segmentation'].value_counts().plot.pie(figsize=(6,6), startangle=0, explode=(0,0,0),\n",
    "                                                     autopct='%.1f%%',shadow=False, colormap ='coolwarm')\n",
    "plt.title(\"Customer Segmentation (RFM Model)\", size=10, fontweight = 'bold')\n",
    "plt.ylabel(\" \")\n",
    "plt.axis('equal') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building - K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize RFM values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rfmScaled = scaler.fit_transform(rfm[['Recency (R)', 'Frequency (F)', 'Monetary (M)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To construct elbow chart to show the optimal number of k\n",
    "\n",
    "inertia = []\n",
    "for k in range (1,11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(rfmScaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-means clustering model with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to the scaled data\n",
    "kmeans.fit(rfmScaled)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Create new column in rfm known as CLuster\n",
    "rfm['Cluster'] = cluster_labels\n",
    "\n",
    "rfm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot containing all three attributes of RFM\n",
    "\n",
    "fig = px.scatter_3d(rfm, x='Monetary (M)', y='Recency (R)', z='Frequency (F)',\n",
    "                    color=cluster_labels, opacity=0.9, size_max=4)\n",
    "\n",
    "fig.update_layout(title='Clustering of Recency, Frequency, and Monetary', \n",
    "                  scene=dict(xaxis_title='Monetary',\n",
    "                             yaxis_title='Recency',\n",
    "                             zaxis_title='Frequency'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterCounts = rfm['Cluster'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(clusterCounts, labels=clusterCounts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Clusters')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of clusters\n",
    "\n",
    "silhouette = silhouette_score(rfmScaled, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results with dataset for further interpretation\n",
    "\n",
    "rfmMerged = pd.merge(newData, rfm[['Customer Segmentation', 'Cluster']], left_on='Order Customer Id', right_index=True, how='right')\n",
    "rfmMerged.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmMerged.to_parquet('rfmMerged.parquet')\n",
    "rfmMerged.to_csv('rfmMerged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy newData into new DataFrame to maintain integrity of original data\n",
    "forecastData = rfmMerged.copy()\n",
    "forecastData['orderDate'] = forecastData['order date (DateOrders)'].dt.date\n",
    "forecastData.head().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print profit by product category over time chart\n",
    "# One cluster per graph\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "forecastData['orderDate'] = pd.to_datetime(forecastData['orderDate'])\n",
    "\n",
    "clusters = forecastData['Cluster'].unique()\n",
    "\n",
    "for clusterValue in clusters:\n",
    "    clusterData = forecastData[forecastData['Cluster'] == clusterValue]\n",
    "    groupedData = clusterData.groupby(['Category Name', pd.Grouper(key='orderDate', freq='M')])['Benefit per order'].sum().reset_index()\n",
    "    pivotData = groupedData.pivot(index='orderDate', columns='Category Name', values='Benefit per order')\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for column in pivotData.columns:\n",
    "        fig.add_trace(go.Scatter(x=pivotData.index, y=pivotData[column], mode='lines', name=column))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Profit by Category Over Time (Cluster {clusterValue})',\n",
    "        xaxis=dict(title='Order Date', tickangle=45),\n",
    "        yaxis=dict(title='Profit'),\n",
    "        legend=dict(title='Category'),\n",
    "        width=1350,\n",
    "        height=700,\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by cluster and category and sum up the benefit per order (profit)\n",
    "clusterCategoryProfit = forecastData.groupby(['Cluster', 'Category Name'])['Benefit per order'].sum().reset_index()\n",
    "\n",
    "sales = forecastData.groupby(['Cluster', 'Category Name'])['Sales per customer'].sum().reset_index()\n",
    "\n",
    "clusterCategoryProfit = clusterCategoryProfit.merge(sales, on=['Cluster', 'Category Name'], how='left')\n",
    "clusterCategoryProfit.rename(columns={'Benefit per order': 'Profit', 'Sales per customer': 'Sales'}, inplace=True)\n",
    "clusterCategoryProfit.tail().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data according to cluster assignment\n",
    "\n",
    "def splitData(data, cluster):\n",
    "    return data[(data['Cluster'] == cluster)]\n",
    "\n",
    "cluster0 = splitData(forecastData, 0)\n",
    "cluster1 = splitData(forecastData, 1)\n",
    "cluster2 = splitData(forecastData, 2)\n",
    "\n",
    "print(\"Cluster 0:\")\n",
    "print(cluster0)\n",
    "print(\"\\nCluster 1:\")\n",
    "print(cluster1)\n",
    "print(\"\\nCluster 2:\")\n",
    "print(cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers and convert date format\n",
    "\n",
    "def processClusterData(clusterData):\n",
    "    \n",
    "    clusterData = clusterData.groupby(['Cluster', 'Sales per customer','Category Name','orderDate'])['Benefit per order'].sum().reset_index()\n",
    "    clusterData.rename(columns={'Benefit per order': 'Profit', 'Sales per customer': 'Sales'}, inplace=True)\n",
    "\n",
    "    # Calculate and remove outliers using IQR method\n",
    "    Q1 = clusterData['Profit'].quantile(0.25)\n",
    "    Q3 = clusterData['Profit'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lowerBound = Q1 - 1.5 * IQR\n",
    "    upperBound = Q3 + 1.5 * IQR\n",
    "    clusterData = clusterData[(clusterData['Profit'] >= lowerBound) & (clusterData['Profit'] <= upperBound)]\n",
    "\n",
    "    # Convert date into year + month and year + week\n",
    "    clusterData['orderDate'] = pd.to_datetime(clusterData['orderDate'])\n",
    "    clusterData['yearMonth'] = clusterData['orderDate'].dt.to_period('M')\n",
    "    clusterData['yearWeek'] = clusterData['orderDate'].dt.to_period('W')\n",
    "    clusterData['yearDay'] = clusterData['orderDate'].dt.to_period('D')\n",
    "\n",
    "    return clusterData\n",
    "\n",
    "# Process data for Cluster 0\n",
    "cluster0 = processClusterData(cluster0)\n",
    "\n",
    "# Process data for Cluster 1\n",
    "cluster1 = processClusterData(cluster1)\n",
    "\n",
    "# Process data for Cluster 2\n",
    "cluster2 = processClusterData(cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_index = False returns pd DataFrame\n",
    "\n",
    "cluster0Profit = cluster0.groupby(['Category Name','Sales','yearWeek'], as_index=False)['Profit'].sum()\n",
    "cluster1Profit = cluster1.groupby(['Category Name','Sales','yearWeek'], as_index=False)['Profit'].sum()\n",
    "cluster2Profit = cluster2.groupby(['Category Name','Sales','yearWeek'], as_index=False)['Profit'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Analysis for Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categories that has less than 14 rows / days of orders as time series decomposition could not be produced\n",
    "\n",
    "grouped = cluster0Profit.groupby('Category Name')\n",
    "\n",
    "categoryCounts = grouped.size()\n",
    "\n",
    "validcategories = categoryCounts[categoryCounts >= 14]\n",
    "\n",
    "cluster0Profit = cluster0Profit[cluster0Profit['Category Name'].isin(validcategories.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition using STL\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "trendList = []\n",
    "\n",
    "for category_name, groupData in cluster0Profit.groupby('Category Name'):\n",
    "    groupData['yearWeek'] = groupData['yearWeek'].dt.to_timestamp()\n",
    "    groupData.set_index('yearWeek', inplace=True)\n",
    "    groupData = groupData[~groupData.index.duplicated()] # Drop duplicate indices if any\n",
    "    groupData = groupData.resample('D').asfreq().fillna(method='ffill') # Resample data to daily frequency & fill missing values\n",
    "    \n",
    "    # Perform STL decomposition\n",
    "    stl = STL(groupData['Profit'], seasonal=7)  # seasonal parameter depends on seasonality of data\n",
    "    result = stl.fit()\n",
    "    \n",
    "    # Save the decomposed components\n",
    "    trend = result.trend\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    # Determine trend type\n",
    "    if trend.diff().dropna().mean() > 0.1:\n",
    "        trendType = 'Increasing'\n",
    "    elif trend.diff().dropna().mean() < -0.1:\n",
    "        trendType = 'Decreasing'\n",
    "    else:\n",
    "        trendType = 'Stable'\n",
    "    \n",
    "    # Store the trend, trend type, seasonal, and residual data\n",
    "    trendData = pd.DataFrame({\n",
    "        'Trend': trend,\n",
    "        'Trend Type': trendType,\n",
    "        'Seasonal': seasonal,\n",
    "        'Residual': residual\n",
    "    })\n",
    "\n",
    "    combinedData = pd.concat([groupData, trendData], axis=1)\n",
    "    trendList.append(combinedData)\n",
    "\n",
    "cluster0Trend = pd.concat(trendList)\n",
    "cluster0Trend['Cluster'] = 0\n",
    "cluster0Trend.to_csv('cluster0Trend.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Analysis for Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = cluster1Profit.groupby('Category Name')\n",
    "\n",
    "categoryCounts = grouped.size()\n",
    "\n",
    "validcategories = categoryCounts[categoryCounts >= 14]\n",
    "\n",
    "cluster1Profit = cluster1Profit[cluster1Profit['Category Name'].isin(validcategories.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendList = []\n",
    "\n",
    "for category_name, groupData in cluster1Profit.groupby('Category Name'):\n",
    "\n",
    "    groupData['yearWeek'] = groupData['yearWeek'].dt.to_timestamp()\n",
    "    groupData.set_index('yearWeek', inplace=True)\n",
    "    groupData = groupData[~groupData.index.duplicated()]\n",
    "    groupData = groupData.resample('D').asfreq().fillna(method='ffill')\n",
    "    \n",
    "\n",
    "    stl = STL(groupData['Profit'], seasonal=7)\n",
    "    result = stl.fit()\n",
    "    \n",
    "    trend = result.trend\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    if trend.diff().dropna().mean() > 0.1:\n",
    "        trendType = 'Increasing'\n",
    "    elif trend.diff().dropna().mean() < -0.1:\n",
    "        trendType = 'Decreasing'\n",
    "    else:\n",
    "        trendType = 'Stable'\n",
    "    \n",
    "    trendData = pd.DataFrame({\n",
    "        'Trend': trend,\n",
    "        'Trend Type': trendType,\n",
    "        'Seasonal': seasonal,\n",
    "        'Residual': residual\n",
    "    })\n",
    "\n",
    "    combinedData = pd.concat([groupData, trendData], axis=1)\n",
    "    \n",
    "    trendList.append(combinedData)\n",
    "\n",
    "cluster1Trend = pd.concat(trendList)\n",
    "cluster1Trend['Cluster'] = 1\n",
    "cluster1Trend.to_csv('cluster1Trend.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Analysis for Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = cluster2Profit.groupby('Category Name')\n",
    "\n",
    "categoryCounts = grouped.size()\n",
    "\n",
    "validcategories = categoryCounts[categoryCounts >= 14]\n",
    "\n",
    "cluster2Profit = cluster2Profit[cluster2Profit['Category Name'].isin(validcategories.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendList = []\n",
    "\n",
    "for category_name, groupData in cluster2Profit.groupby('Category Name'):\n",
    "\n",
    "    groupData['yearWeek'] = groupData['yearWeek'].dt.to_timestamp()\n",
    "    groupData.set_index('yearWeek', inplace=True)\n",
    "    groupData = groupData[~groupData.index.duplicated()]\n",
    "    groupData = groupData.resample('D').asfreq().fillna(method='ffill')\n",
    "\n",
    "    stl = STL(groupData['Profit'], seasonal=7)\n",
    "    result = stl.fit()\n",
    "    \n",
    "    trend = result.trend\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    if trend.diff().dropna().mean() > 0.1:\n",
    "        trendType = 'Increasing'\n",
    "    elif trend.diff().dropna().mean() < -0.1:\n",
    "        trendType = 'Decreasing'\n",
    "    else:\n",
    "        trendType = 'Stable'\n",
    "\n",
    "    trendData = pd.DataFrame({\n",
    "        'Trend': trend,\n",
    "        'Trend Type': trendType,\n",
    "        'Seasonal': seasonal,\n",
    "        'Residual': residual\n",
    "    })\n",
    "\n",
    "    combinedData = pd.concat([groupData, trendData], axis=1)\n",
    "    \n",
    "    trendList.append(combinedData)\n",
    "\n",
    "cluster2Trend = pd.concat(trendList)\n",
    "cluster2Trend['Cluster'] = 2\n",
    "cluster2Trend.to_csv('cluster2Trend.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of all Clusters‘ Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.concat([cluster0Trend, cluster1Trend, cluster2Trend], ignore_index=False)\n",
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.rename(columns={'Category Name': 'categoryName'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeMainCategory(mainCat): \n",
    "    # Define dictionaries mapping category names to main categories \n",
    "    mainCategories = {\n",
    "        'Apparel': ['Baby', 'Men\\'s Apparel', 'Men\\'s Clothing', 'Women\\'s Clothing','Women\\'s Apparel', 'Girls\\' Apparel', 'Children\\'s Clothing', 'Baby ', \n",
    "        'Men\\'s Footwear', 'Women\\'s Footwear', 'Accessories', 'Fitness Accessories', 'Health and Beauty'],\n",
    "        'Sports': ['Sporting Goods', 'Shop By Sport', 'Baseball & Softball', 'Boxing & MMA', 'Camping & Hiking', 'Cardio Equipment', 'Cleats', 'Fishing', \n",
    "        'Golf Apparel', 'Golf Balls', 'Golf Gloves', 'Golf Shoes', 'Golf Bags & Carts', 'Hockey', 'Hunting & Shooting', 'Indoor/Outdoor Games', 'Lacrosse', \n",
    "        'Tennis & Racquet', 'Water Sports', 'Soccer', 'Basketball', 'Strength Training', 'Kids\\' Golf Clubs', 'Men\\'s Golf Clubs', 'Women\\'s Golf Clubs', 'Garden'],\n",
    "        'Electronics': ['Electronics', 'Cameras ', 'Computers', 'Consumer Electronics'],\n",
    "        'Entertainment': ['As Seen on  TV!','Books ', 'CDs ', 'DVDs', 'Music', 'Video Games', 'Toys', 'Crafts', 'As Seen on TV!', 'Trade-In', 'Pet Supplies']\n",
    "    }\n",
    "    \n",
    "    # Function to map category to main category \n",
    "    def mapToMainCategory(categoryName): \n",
    "        for mainCategory, categories in mainCategories.items(): \n",
    "            if categoryName in categories: \n",
    "                return mainCategory \n",
    "        return 'Other' \n",
    "    \n",
    "    # Apply the mapping function to the categoryName column \n",
    "    mainCat['mainCategory'] = mainCat['categoryName'].apply(mapToMainCategory) \n",
    "    return mainCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorizeMainCategory(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.to_csv('trendAnalysis.csv')\n",
    "combine.to_parquet('trendAnalysis.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profit Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.time_series import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('trendAnalysis.csv')\n",
    "data['yearWeek'] = pd.to_datetime(data['yearWeek'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\"Cluster\", \"Category Name\", \"yearWeek\"]\n",
    "\n",
    "aggData = data.groupby(group).sum()\n",
    "aggData.reset_index(inplace=True)\n",
    "aggData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggData.rename(columns={'Category Name': 'categoryName'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.time_series import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "resultsCompilation = pd.DataFrame()\n",
    "evaluationMetrics = pd.DataFrame()\n",
    "\n",
    "with tqdm(range(len(aggData.groupby([\"Cluster\", \"categoryName\"])))) as pbar:\n",
    "    for _, data in aggData.groupby([\"Cluster\", \"categoryName\"]):\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        target_df = data.set_index('yearWeek')[['Profit']]\n",
    "        cluster = data.Cluster[0]\n",
    "        category = data.categoryName[0]\n",
    "        num = len(data)* 0.1\n",
    "        num = round(num)\n",
    "\n",
    "        s = setup(target_df, target='Profit', verbose=False, fh = num)\n",
    "\n",
    "        best = compare_models(include = ['arima'], verbose=False)\n",
    "        evaluationMetric = pull()\n",
    "\n",
    "        predictions = predict_model(best, fh = 365)\n",
    "        predictions.reset_index(inplace=True)\n",
    "        predictions['yearWeek'] = predictions['index'].dt.to_timestamp()\n",
    "        predictions.drop(columns=['index'], inplace=True)\n",
    "        predictions[\"Cluster\"] = cluster\n",
    "        predictions[\"categoryName\"] = category\n",
    "        predictions[\"algorithm\"] = str(best)\n",
    "\n",
    "        results = pd.concat([data, predictions])\n",
    "        resultsCompilation = pd.concat([resultsCompilation, results], ignore_index=True)\n",
    "        evaluationMetrics = pd.concat([evaluationMetrics, evaluationMetric], ignore_index=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "# Saving error metrics and score to a CSV file\n",
    "evaluationMetrics.to_csv('evaluationMetrics_arimaa.csv', index=False)\n",
    "resultsCompilation.to_csv('resultsCompilation_arimaa.csv')\n",
    "resultsCompilation.to_parquet('resultsCompilation_arimaa.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.time_series import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "resultsCompilation = pd.DataFrame()\n",
    "evaluationMetrics = pd.DataFrame()\n",
    "\n",
    "with tqdm(range(len(aggData.groupby([\"Cluster\", \"categoryName\"])))) as pbar:\n",
    "    for _, data in aggData.groupby([\"Cluster\", \"categoryName\"]):\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        target_df = data.set_index('yearWeek')[['Profit']]\n",
    "        cluster = data.Cluster[0]\n",
    "        category = data.categoryName[0]\n",
    "        num = len(data)* 0.1\n",
    "        num = round(num)\n",
    "\n",
    "        s = setup(target_df, target='Profit', verbose=False, fh = num)\n",
    "\n",
    "        best = compare_models(include = ['lightgbm_cds_dt'], verbose=False)\n",
    "        evaluationMetric = pull()\n",
    "\n",
    "        predictions = predict_model(best, fh = 365)\n",
    "        predictions.reset_index(inplace=True)\n",
    "        predictions['yearWeek'] = predictions['index'].dt.to_timestamp()\n",
    "        predictions.drop(columns=['index'], inplace=True)\n",
    "        predictions[\"Cluster\"] = cluster\n",
    "        predictions[\"categoryName\"] = category\n",
    "        predictions[\"algorithm\"] = str(best)\n",
    "\n",
    "        results = pd.concat([data, predictions])\n",
    "        resultsCompilation = pd.concat([resultsCompilation, results], ignore_index=True)\n",
    "        evaluationMetrics = pd.concat([evaluationMetrics, evaluationMetric], ignore_index=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "# Saving error metrics and score to a CSV file\n",
    "evaluationMetrics.to_csv('evaluationMetrics_gbr.csv', index=False)\n",
    "resultsCompilation.to_csv('resultsCompilation_gbr.csv')\n",
    "resultsCompilation.to_parquet('resultsCompilation_gbr.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.time_series import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "resultsCompilation = pd.DataFrame()\n",
    "evaluationMetrics = pd.DataFrame()\n",
    "\n",
    "with tqdm(range(len(aggData.groupby([\"Cluster\", \"categoryName\"])))) as pbar:\n",
    "    for _, data in aggData.groupby([\"Cluster\", \"categoryName\"]):\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        target_df = data.set_index('yearWeek')[['Profit']]\n",
    "        cluster = data.Cluster[0]\n",
    "        category = data.categoryName[0]\n",
    "        num = len(data)* 0.1\n",
    "        num = round(num)\n",
    "\n",
    "        s = setup(target_df, target='Profit', verbose=False, fh = num)\n",
    "\n",
    "        best = compare_models(include = ['rf_cds_dt'], verbose=False)\n",
    "        evaluationMetric = pull()\n",
    "\n",
    "        predictions = predict_model(best, fh = 365)\n",
    "        predictions.reset_index(inplace=True)\n",
    "        predictions['yearWeek'] = predictions['index'].dt.to_timestamp()\n",
    "        predictions.drop(columns=['index'], inplace=True)\n",
    "        predictions[\"Cluster\"] = cluster\n",
    "        predictions[\"categoryName\"] = category\n",
    "        predictions[\"algorithm\"] = str(best)\n",
    "\n",
    "        results = pd.concat([data, predictions])\n",
    "        resultsCompilation = pd.concat([resultsCompilation, results], ignore_index=True)\n",
    "        evaluationMetrics = pd.concat([evaluationMetrics, evaluationMetric], ignore_index=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "# Saving error metrics and score to a CSV file\n",
    "evaluationMetrics.to_csv('evaluationMetrics_rf.csv', index=False)\n",
    "resultsCompilation.to_csv('resultsCompilation_rf.csv')\n",
    "resultsCompilation.to_parquet('resultsCompilation_rf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.time_series import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "resultsCompilation = pd.DataFrame()\n",
    "evaluationMetrics = pd.DataFrame()\n",
    "\n",
    "with tqdm(range(len(aggData.groupby([\"Cluster\", \"categoryName\"])))) as pbar:\n",
    "    for _, data in aggData.groupby([\"Cluster\", \"categoryName\"]):\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        target_df = data.set_index('yearWeek')[['Profit']]\n",
    "        cluster = data.Cluster[0]\n",
    "        category = data.categoryName[0]\n",
    "        num = len(data)* 0.1\n",
    "        num = round(num)\n",
    "\n",
    "        s = setup(target_df, target='Profit', verbose=False, fh = num)\n",
    "\n",
    "        best = compare_models(include = ['lr_cds_dt'], verbose=False)\n",
    "        evaluationMetric = pull()\n",
    "\n",
    "        predictions = predict_model(best, fh = 365)\n",
    "        predictions.reset_index(inplace=True)\n",
    "        predictions['yearWeek'] = predictions['index'].dt.to_timestamp()\n",
    "        predictions.drop(columns=['index'], inplace=True)\n",
    "        predictions[\"Cluster\"] = cluster\n",
    "        predictions[\"categoryName\"] = category\n",
    "        predictions[\"algorithm\"] = str(best)\n",
    "\n",
    "        results = pd.concat([data, predictions])\n",
    "        resultsCompilation = pd.concat([resultsCompilation, results], ignore_index=True)\n",
    "        evaluationMetrics = pd.concat([evaluationMetrics, evaluationMetric], ignore_index=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "# Saving error metrics and score to a CSV file\n",
    "evaluationMetrics.to_csv('evaluationMetrics_lr.csv', index=False)\n",
    "resultsCompilation.to_csv('resultsCompilation_lr.csv')\n",
    "resultsCompilation.to_parquet('resultsCompilation_lr.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting of Product Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pd.read_csv('evaluationMetrics_rf.csv')\n",
    "lr = pd.read_csv('evaluationMetrics_lr.csv')\n",
    "arima = pd.read_csv('evaluationMetrics_arima.csv')\n",
    "lgb = pd.read_csv('evaluationMetrics_lightgbm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf['Model'] = 'Random Forest'\n",
    "lr['Model'] = 'Linear Regression'\n",
    "arima['Model'] = 'ARIMA'\n",
    "lgb['Model'] = 'LightGBM'\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combinedError = pd.concat([rf, lgb, lr, arima])\n",
    "\n",
    "# Group by 'Model' and calculate the mean\n",
    "eva = combinedError.groupby('Model')['MAE', 'RMSE'].mean()\n",
    "eva.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
